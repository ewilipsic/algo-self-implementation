{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab11 Logistic :Regression and k-Nearest Neighbors (kNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X):\n",
    "    sum = np.sum(X,axis=0)\n",
    "    sum = sum / X.shape[0]\n",
    "    X = X - sum\n",
    "    standard_deviation = np.sqrt(np.sum(np.square(X),axis=0)/X.shape[0])\n",
    "    return X / standard_deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "Dataset is loaded and then equal percentage(80%) of both malignant and benign patients are added to the training set and the rest(20%) to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_21616\\1089245081.py:10: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_malignant = gp.get_group(1)\n",
      "C:\\Users\\ayush\\AppData\\Local\\Temp\\ipykernel_21616\\1089245081.py:11: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  data_benign = gp.get_group(0)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"wbdc.csv\")\n",
    "def replM(df): return df != 'M'\n",
    "def replB(df): return df != 'B'\n",
    "\n",
    "# Replace M and B with 1 and 0\n",
    "data = data.where(replM,1)\n",
    "data = data.where(replB,0)\n",
    "\n",
    "gp = data.groupby(['diagnosis'])\n",
    "data_malignant = gp.get_group(1)\n",
    "data_benign = gp.get_group(0)\n",
    "\n",
    "X = data_malignant.iloc[:,2:].to_numpy(np.float64)\n",
    "Y = data_malignant.iloc[:,1].to_numpy(np.float64)\n",
    "\n",
    "X_train,X_test = X[0:(X.shape[0]*8)//10],X[(X.shape[0]*8)//10:]\n",
    "Y_train,Y_test = Y[0:(X.shape[0]*8)//10],Y[(X.shape[0]*8)//10:]\n",
    "\n",
    "X = data_benign.iloc[:,2:].to_numpy(np.float64)\n",
    "Y = data_benign.iloc[:,1].to_numpy(np.float64)\n",
    "\n",
    "X_train = np.concatenate((X_train,X[0:(X.shape[0]*8)//10]),axis = 0)\n",
    "X_test = np.concatenate((X_test,X[(X.shape[0]*8)//10:]),axis = 0)\n",
    "\n",
    "Y_train = np.concatenate((Y_train,Y[0:(Y.shape[0]*8)//10]),axis = 0)\n",
    "Y_test = np.concatenate((Y_test,Y[(Y.shape[0]*8)//10:]),axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): # return numpy array with element wise operations\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def initialize_weights(n_features):\n",
    "    weights = np.zeros(n_features)\n",
    "    bias  = np.zeros(1)\n",
    "    return weights,bias\n",
    "\n",
    "def compute_cost(X,y,weights,bias):\n",
    "    Y = sigmoid(X.dot(weights) + bias)\n",
    "    SUM = np.sum(y * np.log10(Y) + (1-y) * np.log10(1-Y))\n",
    "    return -SUM/y.shape[0]\n",
    "\n",
    "def optimize_weights(X, y, weights, bias, learning_rate, num_iterations):\n",
    "    for epoch in range(num_iterations):\n",
    "        pred = sigmoid(X.dot(weights) + bias)\n",
    "        grad = np.matmul(X.T,(y/pred - (1-y)/(1-pred)) * (pred * (1-pred))) * (-1/X.shape[0])\n",
    "        #print(grad)\n",
    "        weights = weights - grad * learning_rate\n",
    "        bias_grad = np.sum(((y/pred - (1-y)/(1-pred)) * (pred * (1-pred))) * (-1/X.shape[0]))\n",
    "        bias = bias - bias_grad * learning_rate\n",
    "\n",
    "        \n",
    "        print(f\"epoch {epoch + 1} training loss : {compute_cost(X,y,weights,bias)}\")\n",
    "    return weights,bias\n",
    "\n",
    "def train_logistic_regression(X, y, learning_rate, num_iterations):\n",
    "    weights,bias = initialize_weights(X.shape[1])\n",
    "    return optimize_weights(X, y,weights,bias,learning_rate, num_iterations)\n",
    "    \n",
    "def predict_logistic_regression(X, weights, bias):\n",
    "    Y = sigmoid(X.dot(weights) + bias)\n",
    "    Y = np.where(Y < 0.5,0.0,1.0)\n",
    "    return Y\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    correct = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            correct+=1\n",
    "    return correct/y_true.shape[0] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss : 0.31304369587060604\n",
      "epoch 2 training loss : 0.34802431757763413\n",
      "epoch 3 training loss : 0.44298018286057994\n",
      "epoch 4 training loss : 0.3866376083594666\n",
      "epoch 5 training loss : 0.48830663566081756\n",
      "epoch 6 training loss : 0.3586702930990497\n",
      "epoch 7 training loss : 0.4603060594788346\n",
      "epoch 8 training loss : 0.35963017235303957\n",
      "epoch 9 training loss : 0.4608668470705603\n",
      "epoch 10 training loss : 0.3494025746180105\n",
      "epoch 11 training loss : 0.4478627616763778\n",
      "epoch 12 training loss : 0.34467197660628385\n",
      "epoch 13 training loss : 0.4405698274067052\n",
      "epoch 14 training loss : 0.3381181634628196\n",
      "epoch 15 training loss : 0.4303376155460528\n",
      "epoch 16 training loss : 0.3328806045253153\n",
      "epoch 17 training loss : 0.4212911961115972\n",
      "epoch 18 training loss : 0.32751550522328665\n",
      "epoch 19 training loss : 0.4116904978191488\n",
      "epoch 20 training loss : 0.32257083990735813\n",
      "epoch 21 training loss : 0.40241480012999714\n",
      "epoch 22 training loss : 0.31775683843522917\n",
      "epoch 23 training loss : 0.3931675629977728\n",
      "epoch 24 training loss : 0.31313751072720236\n",
      "epoch 25 training loss : 0.3841183574007848\n",
      "epoch 26 training loss : 0.308649793093435\n",
      "epoch 27 training loss : 0.3752330468029876\n",
      "epoch 28 training loss : 0.30428502795293066\n",
      "epoch 29 training loss : 0.3665400036177974\n",
      "epoch 30 training loss : 0.30002223736271466\n",
      "epoch 31 training loss : 0.35803891442206504\n",
      "epoch 32 training loss : 0.29584842041678977\n",
      "epoch 33 training loss : 0.3497340688238079\n",
      "epoch 34 training loss : 0.2917514494445018\n",
      "epoch 35 training loss : 0.34162617414784846\n",
      "epoch 36 training loss : 0.2877212785222572\n",
      "epoch 37 training loss : 0.3337154860493367\n",
      "epoch 38 training loss : 0.28374922971614897\n",
      "epoch 39 training loss : 0.32600152710829894\n",
      "epoch 40 training loss : 0.27982784338083944\n",
      "epoch 41 training loss : 0.31848331949261083\n",
      "epoch 42 training loss : 0.27595069268592787\n",
      "epoch 43 training loss : 0.3111593975782487\n",
      "epoch 44 training loss : 0.27211224535987316\n",
      "epoch 45 training loss : 0.3040278208239872\n",
      "epoch 46 training loss : 0.268307749011681\n",
      "epoch 47 training loss : 0.29708620779381484\n",
      "epoch 48 training loss : 0.26453314422232654\n",
      "epoch 49 training loss : 0.2903318256636906\n",
      "epoch 50 training loss : 0.26078501514177294\n",
      "epoch 51 training loss : 0.28376175783404567\n",
      "epoch 52 training loss : 0.2570605931999932\n",
      "epoch 53 training loss : 0.27737316620358404\n",
      "epoch 54 training loss : 0.25335783334437595\n",
      "epoch 55 training loss : 0.27116365903178047\n",
      "epoch 56 training loss : 0.2496755835762126\n",
      "epoch 57 training loss : 0.2651317687154769\n",
      "epoch 58 training loss : 0.24601386627569538\n",
      "epoch 59 training loss : 0.2592775340175686\n",
      "epoch 60 training loss : 0.24237428198215222\n",
      "epoch 61 training loss : 0.25360316543425454\n",
      "epoch 62 training loss : 0.23876053047422327\n",
      "epoch 63 training loss : 0.2481137474630867\n",
      "epoch 64 training loss : 0.23517901789483028\n",
      "epoch 65 training loss : 0.24281789609345486\n",
      "epoch 66 training loss : 0.23163948195042192\n",
      "epoch 67 training loss : 0.23772824696312186\n",
      "epoch 68 training loss : 0.2281555242010311\n",
      "epoch 69 training loss : 0.23286161025643737\n",
      "epoch 70 training loss : 0.22474490050102805\n",
      "epoch 71 training loss : 0.22823861247836194\n",
      "epoch 72 training loss : 0.22142940534224476\n",
      "epoch 73 training loss : 0.22388267651079166\n",
      "epoch 74 training loss : 0.2182342102763354\n",
      "epoch 75 training loss : 0.21981828435412762\n",
      "epoch 76 training loss : 0.21518658611008096\n",
      "epoch 77 training loss : 0.21606861149717868\n",
      "epoch 78 training loss : 0.21231403973802257\n",
      "epoch 79 training loss : 0.2126527794107382\n",
      "epoch 80 training loss : 0.209642004120438\n",
      "epoch 81 training loss : 0.20958309438169145\n",
      "epoch 82 training loss : 0.20719131552861222\n",
      "epoch 83 training loss : 0.2068626960355604\n",
      "epoch 84 training loss : 0.20497579373283112\n",
      "epoch 85 training loss : 0.2044840251738802\n",
      "epoch 86 training loss : 0.2030003073659545\n",
      "epoch 87 training loss : 0.20242844473832008\n",
      "epoch 88 training loss : 0.20125973124409113\n",
      "epoch 89 training loss : 0.20066720266971785\n",
      "epoch 90 training loss : 0.1997391294898559\n",
      "epoch 91 training loss : 0.19916370000630854\n",
      "epoch 92 training loss : 0.19841528536337422\n",
      "epoch 93 training loss : 0.1978767464891666\n",
      "epoch 94 training loss : 0.19725937368247937\n",
      "epoch 95 training loss : 0.1967642336415713\n",
      "epoch 96 training loss : 0.1962402502951127\n",
      "epoch 97 training loss : 0.19578654396279066\n",
      "epoch 98 training loss : 0.19532766404460447\n",
      "epoch 99 training loss : 0.1949091112460105\n",
      "epoch 100 training loss : 0.19449476653793238\n",
      "train accuracy Logistic Regression:  0.9140969162995595\n",
      "test accuracy Logistic Regression:  0.8869565217391304\n"
     ]
    }
   ],
   "source": [
    "weights,bias = train_logistic_regression(X_train,Y_train,0.0000082,100)\n",
    "print(\"train accuracy Logistic Regression: \",accuracy(predict_logistic_regression(X_train,weights,bias),Y_train))\n",
    "print(\"test accuracy Logistic Regression: \",accuracy(predict_logistic_regression(X_test,weights,bias),Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1,x2):\n",
    "    return np.sqrt(np.sum(np.square(x1-x2))).item()\n",
    "\n",
    "def get_neighbors(X_train, X_test_instance, k):\n",
    "    distances = np.array([euclidean_distance(X_test_instance,x2) for x2 in X_train])\n",
    "    args = np.argsort(distances)\n",
    "    return args[:k]\n",
    "\n",
    "def predict_kNN(X_train,y_train,X_test,k):\n",
    "    preds = []\n",
    "    for X_test_instance in X_test:\n",
    "        neighbors = get_neighbors(X_train,X_test_instance,k)\n",
    "        ones = 0\n",
    "        for idx in neighbors: \n",
    "            if y_train[idx] == 1: ones+=1\n",
    "        if(ones > k//2): preds.append(1.0)\n",
    "        else : preds.append(0.0)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test kNN accuracy for k  = 1:  0.9043478260869565\n",
      "test kNN accuracy for k  = 3:  0.9043478260869565\n",
      "test kNN accuracy for k  = 5:  0.9217391304347826\n",
      "test kNN accuracy for k  = 7:  0.9130434782608695\n",
      "test kNN accuracy for k  = 9:  0.9304347826086956\n",
      "test kNN accuracy for k  = 11:  0.9217391304347826\n",
      "test kNN accuracy for k  = 13:  0.9391304347826087\n",
      "test kNN accuracy for k  = 15:  0.9391304347826087\n"
     ]
    }
   ],
   "source": [
    "for k in range(1,16,2):    \n",
    "    Y_test_pred = predict_kNN(X_train,Y_train,X_test,k)\n",
    "    print(f\"test kNN accuracy for k  = {k}: \",accuracy(Y_test_pred,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 â€“ Which model performs better and why?<br>\n",
    "Ans - It can be seen that knn is able to achieve a higher accuracy of the test data.<br>\n",
    "Which can be because a non-linear decision boundary might be more suitable for the data.<br>\n",
    "\n",
    "Q2 - How does the choice of k affect the performance of kNN?<br>\n",
    "Ans - The range of values of k from 5-15 produce very similiar result.<br>But values 1 and 3 produce slightly worse result.<br>Best result is seen for k = 13,15.<br>\n",
    "\n",
    "Q3 - What are the strengths and limitations of Logistic Regression and kNN for this classification problem?<br>\n",
    "Ans - <br>\n",
    "Logistic Regression:<br>\n",
    "Strengths: Efficient, good for high-dimensional data.<br>\n",
    "Limitations: Assumes linear relationships, sensitive to outliers.<br>\n",
    "\n",
    "kNN:<br>\n",
    "Strengths: Simple to implement,can deal with non-linear decision boundaries.<br>\n",
    "Limitations: Computationally expensive, sensitive to irrelevant features<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
